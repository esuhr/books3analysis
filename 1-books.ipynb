{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Books3 Dataset\n",
    "datasource: https://sites.google.com/eng.ucsd.edu/ucsdbookgraph/home\n",
    "\n",
    "For this dataset, we'll be scraping copyright tags and filenames for each of the 193,383 books in the dataset.\n",
    "In part 2, we'll use string matching to match the scraped filenames to the Goodreads dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "# create list of all characters 0-9, A-Z\n",
    "characters = [chr(i) for i in range(48, 58)] + [chr(i) for i in range(65, 91)]\n",
    "\n",
    "characters.append('0_Other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "alltitles = []\n",
    "for character in characters:\n",
    "    directorypath = './Bibliotik/{}'.format(character)\n",
    "    filenames = os.listdir(directorypath.format(character))\n",
    "    for filename in filenames:\n",
    "        alltitles.append(character+ '/' + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filepath</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0/059600298X_CPP.epub.txt</td>\n",
       "      <td>059600298X_CPP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0/01 - Alec Dunn.epub.txt</td>\n",
       "      <td>01 - Alec Dunn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0/03 Cole_ Ninja Of Earth (Scholastic) - Greg ...</td>\n",
       "      <td>03 Cole_ Ninja Of Earth (Scholastic) - Greg Fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0/0596006977_CNutshell.epub.txt</td>\n",
       "      <td>0596006977_CNutshell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0/091 - The Berenstain Bears and the In-Crowd ...</td>\n",
       "      <td>091 - The Berenstain Bears and the In-Crowd - ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            filepath  \\\n",
       "0                          0/059600298X_CPP.epub.txt   \n",
       "1                          0/01 - Alec Dunn.epub.txt   \n",
       "2  0/03 Cole_ Ninja Of Earth (Scholastic) - Greg ...   \n",
       "3                    0/0596006977_CNutshell.epub.txt   \n",
       "4  0/091 - The Berenstain Bears and the In-Crowd ...   \n",
       "\n",
       "                                               title  \n",
       "0                                     059600298X_CPP  \n",
       "1                                     01 - Alec Dunn  \n",
       "2  03 Cole_ Ninja Of Earth (Scholastic) - Greg Fa...  \n",
       "3                               0596006977_CNutshell  \n",
       "4  091 - The Berenstain Bears and the In-Crowd - ...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(alltitles, columns=['filepath'])\n",
    "\n",
    "df['title'] = df['filepath'].apply(lambda x: x.replace('.epub.txt', ''))\n",
    "df['title'] = df['title'].apply(lambda x: x.split('/', 1)[1])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "copyrighttags = ['COPYRIGHT', '©']\n",
    "# find copyright in each file and print surrounding lines\n",
    "def find_cpright(filepath):\n",
    "    with open(filepath, 'r') as f:\n",
    "        text = f.read()\n",
    "        for tag in copyrighttags:\n",
    "            matches = re.findall(tag, text, re.IGNORECASE)\n",
    "            if matches:\n",
    "                matchtexts = []\n",
    "                for match in matches:\n",
    "                    start = text.index(match)\n",
    "                    end = min(start + 100, len(text))\n",
    "                    matchtexts.append(text[start:end])\n",
    "                return [filepath, matchtexts]\n",
    "            else:\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "directorypath = './Bibliotik/{}'\n",
    "pathlist = df['filepath'].to_list()\n",
    "\n",
    "cprightlist = []\n",
    "\n",
    "for path in pathlist:\n",
    "    filepath = find_cpright(directorypath.format(path))\n",
    "    cprightlist.append(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cprightlist.pkl', 'wb') as f:\n",
    "    pickle.dump(cprightlist, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "It's important to note that while Spacy is fairly accurate, it's not perfect. \n",
    "In their documentation, their largest language model 'en_core_web_trf' has a 90% accuracy rate on Named Entity Recognition tasks.\n",
    "This accuracy rate could introduce bias into our analysis. For example, depending on how the model is trained, it may be more likely to identify common English names as people.\n",
    "This would bias our data towards English names with non-English names less likely to be recognized as people.\n",
    "However, for the scope of this project, we will work with the assumption that the model is accurate enough for our purposes.\n",
    "\n",
    "'''\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_trf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_entities(item):\n",
    "    entitydict = {\n",
    "        'author': '',\n",
    "        'title': '',\n",
    "        'copyright': [],\n",
    "        'copyrightYear': '',\n",
    "        'filepath': '',\n",
    "    }\n",
    "\n",
    "    fp, texts = item\n",
    "\n",
    "    fp_stripped = fp.replace('.epub.txt', '').replace('./Bibliotik/', '').split('/', )[1]\n",
    "    fp_stripped = fp_stripped.replace('_', ' ').replace('[', '').replace(']', '')\n",
    "    title, author = fp_stripped.rsplit(' - ', 1) if ' - ' in fp_stripped else (fp_stripped, 'Unknown')\n",
    "\n",
    "    if nlp(author).ents and nlp(author).ents[0].label_ in {'PERSON', 'ORG'}:\n",
    "        entitydict['author'] = nlp(author).ents[0].text\n",
    "        entitydict['title'] = title\n",
    "    elif nlp(title).ents and nlp(title).ents[0].label_ in {'PERSON', 'ORG'}:\n",
    "        entitydict['author'] = nlp(title).ents[0].text\n",
    "        entitydict['title'] = author\n",
    "    else:\n",
    "        entitydict['author'] = author\n",
    "        entitydict['title'] = title\n",
    "\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "\n",
    "    for text in texts:\n",
    "        text = text.replace('\\n', ' ')\n",
    "        textdoc = nlp(text)\n",
    "        for ent in textdoc.ents:\n",
    "            if ent.label_ in {'PERSON', 'ORG'} and any(tag in text[ent.start_char-12:ent.start_char] for tag in {'COPYRIGHT', '©', 'COPYRIGHT'}):\n",
    "                entitydict['copyright'].append(ent.text)\n",
    "            elif ent.label_ == 'DATE' and any(tag in text[ent.start_char-12:ent.start_char] for tag in {'COPYRIGHT', '©', 'COPYRIGHT'}):\n",
    "                entitydict['copyrightYear'] = ent.text\n",
    "\n",
    "    entitydict['filepath'] = fp\n",
    "    return entitydict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "entitylist = []\n",
    "with open('entitylist.txt') as f:\n",
    "    for line in f:\n",
    "        entitylist.append(eval(line))\n",
    "\n",
    "entity_df = pd.DataFrame(entitylist)\n",
    "entity_df.to_parquet('books3copyright.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
